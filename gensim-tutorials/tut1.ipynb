{
 "metadata": {
  "name": "",
  "signature": "sha256:825c5fc3e44bd9ac8fbf4ab53b1e2a7ab788b557f7fc56a5365e5af4e79812ab"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents = [\"Human machine interface for lab abc computer applications\",\n",
      "              \"A survey of user opinion of computer system response time\",\n",
      "              \"The EPS user interface management system\",\n",
      "              \"System and human system engineering testing of EPS\",\n",
      "              \"Relation of user perceived response time to error measurement\",\n",
      "              \"The generation of random binary unordered trees\",\n",
      "              \"The intersection graph of paths in trees\",\n",
      "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
      "              \"Graph minors A survey\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove common words and tokenize\n",
      "stoplist = set('for a of the and to in'.split())\n",
      "texts = [[word for word in document.lower().split() if word not in stoplist] \n",
      "          for document in documents]\n",
      "\n",
      "# remove words that appear only once\n",
      "from collections import defaultdict\n",
      "frequency = defaultdict(int)\n",
      "for text in texts:\n",
      "    for token in text:\n",
      "        frequency[token] += 1\n",
      "        \n",
      "texts = [[token for token in text if frequency[token] > 1]\n",
      "         for text in texts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pprint import pprint   # pretty-printer\n",
      "pprint(texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['human', 'interface', 'computer'],\n",
        " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
        " ['eps', 'user', 'interface', 'system'],\n",
        " ['system', 'human', 'system', 'eps'],\n",
        " ['user', 'response', 'time'],\n",
        " ['trees'],\n",
        " ['graph', 'trees'],\n",
        " ['graph', 'minors', 'trees'],\n",
        " ['graph', 'minors', 'survey']]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary = corpora.Dictionary(texts)\n",
      "dictionary.save('/tmp/deerwester.dict') # store the dictionary, for future reference\n",
      "print(dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " print(dictionary.token2id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'minors': 11, u'graph': 10, u'system': 6, u'trees': 9, u'eps': 8, u'computer': 1, u'survey': 5, u'user': 7, u'human': 2, u'time': 4, u'interface': 0, u'response': 3}\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_doc = \"Human computer interaction\"\n",
      "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
      "print(new_vec) # the word \"interaction\" does not appear in the dictionary and is ignored"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(1, 1), (2, 1)]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus) # store to disk, for later use\n",
      "print(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[(0, 1), (1, 1), (2, 1)], [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(0, 1), (6, 1), (7, 1), (8, 1)], [(2, 1), (6, 2), (8, 1)], [(3, 1), (4, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(5, 1), (10, 1), (11, 1)]]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MyCorpus(object):\n",
      "    def __iter__(self):\n",
      "        for line in open('mycorpus.txt'):\n",
      "            # assume there's one document per line, tokens separated by whitespace\n",
      "            yield dictionary.doc2bow(line.lower().split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!\n",
      "print(corpus_memory_friendly)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<__main__.MyCorpus object at 0x108e5fb90>\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for vector in corpus_memory_friendly: # load one vector into memory at a time\n",
      "    print(vector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 1), (1, 1), (2, 1)]\n",
        "[(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
        "[(0, 1), (6, 1), (7, 1), (8, 1)]\n",
        "[(2, 1), (6, 2), (8, 1)]\n",
        "[(3, 1), (4, 1), (7, 1)]\n",
        "[(9, 1)]\n",
        "[(9, 1), (10, 1)]\n",
        "[(9, 1), (10, 1), (11, 1)]\n",
        "[(5, 1), (10, 1), (11, 1)]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# collect statistics about all tokens\n",
      "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))\n",
      "# remove stop words and words that appear only once\n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
      "            if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
      "dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
      "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "print(dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora\n",
      "# create a toy corpus of 2 documents, as a plain Python list\n",
      "corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it\n",
      "\n",
      "corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpora.SvmLightCorpus.serialize('/tmp/corpus.svmlight', corpus)\n",
      "corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)\n",
      "corpora.LowCorpus.serialize('/tmp/corpus.low', corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.corpora.lowcorpus:List-of-words format can only save vectors with integer elements; 1 float entries were truncated to integer value\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = corpora.MmCorpus('/tmp/corpus.mm')\n",
      "print(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MmCorpus(2 documents, 2 features, 1 non-zero entries)\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}