{
 "metadata": {
  "name": "",
  "signature": "sha256:85850889793a98e0195f4b2e337552965ee7b1f93309b0e30ae740b22547b06b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer(min_df=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=None, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "content = [\"How to format my hard dist\", \" Hard disk format problems \"]\n",
      "X = vectorizer.fit_transform(content)\n",
      "vectorizer.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[u'disk', u'dist', u'format', u'hard', u'how', u'my', u'problems', u'to']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.toarray().transpose()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([[0, 1],\n",
        "       [1, 0],\n",
        "       [1, 1],\n",
        "       [1, 1],\n",
        "       [1, 0],\n",
        "       [1, 0],\n",
        "       [0, 1],\n",
        "       [1, 0]])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "DIR = './text'\n",
      "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer(min_df=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.\\n',\n",
        " 'Imaging databases can get huge.\\n',\n",
        " 'Most imaging databases safe images permanently.\\n',\n",
        " 'Imaging databases store images.\\n',\n",
        " 'Imaging databases store images. Imaging databases store images. Imaging databases store images.\\n']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(posts)\n",
      "num_samples, num_features = X_train.shape\n",
      "print(\"#samples: %d, #features: %d\" %(num_samples, num_features))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 5, #features: 24\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[u'about',\n",
        " u'actually',\n",
        " u'can',\n",
        " u'contains',\n",
        " u'databases',\n",
        " u'get',\n",
        " u'huge',\n",
        " u'images',\n",
        " u'imaging',\n",
        " u'interesting',\n",
        " u'is',\n",
        " u'it',\n",
        " u'learning',\n",
        " u'machine',\n",
        " u'most',\n",
        " u'much',\n",
        " u'not',\n",
        " u'permanently',\n",
        " u'post',\n",
        " u'safe',\n",
        " u'store',\n",
        " u'stuff',\n",
        " u'this',\n",
        " u'toy']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_post = \"imaging databases\"\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "print(new_post_vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 4)\t1\n",
        "  (0, 8)\t1\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_post_vec.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "array([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0]])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "def dist_raw(v1, v2):\n",
      "    delta = v1 - v2\n",
      "    return sp.linalg.norm(delta.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "\n",
      "best_doc = None\n",
      "best_dist = sys.maxint\n",
      "best_i = None\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist_raw(post_vec, new_post_vec)\n",
      "    print \"=== Post %i with dist=%.2f: %s\" % (i, d, post)\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "        \n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "\n",
        "=== Post 1 with dist=1.73: Imaging databases can get huge.\n",
        "\n",
        "=== Post 2 with dist=2.00: Most imaging databases safe images permanently.\n",
        "\n",
        "=== Post 3 with dist=1.41: Imaging databases store images.\n",
        "\n",
        "=== Post 4 with dist=5.10: Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
        "\n",
        "Best post is 3 with dist=1.41\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#  normalize \n",
      "# \u305d\u306e\u307e\u307e\u6bd4\u8f03\u3059\u308b\u3068\u6587\u7ae0\u306e\u9577\u3055\uff08\u5358\u8a9e\u306e\u91cf\uff09\u3067\u7d50\u679c\u304c\u5de6\u53f3\u3055\u308c\u308b\u306e\u3067\u6b63\u898f\u5316\uff08\u5358\u8a9e\u6570\u3067\u5272\u308b)\n",
      "def dist_norm(v1, v2):\n",
      "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
      "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
      "    delta = v1_normalized - v2_normalized\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "best_doc = None\n",
      "best_dist = sys.maxint\n",
      "best_i = None\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist_norm(post_vec, new_post_vec)\n",
      "    print \"=== Post %i with dist=%.2f: %s\" % (i, d, post)\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "        \n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "\n",
        "=== Post 1 with dist=0.86: Imaging databases can get huge.\n",
        "\n",
        "=== Post 2 with dist=0.92: Most imaging databases safe images permanently.\n",
        "\n",
        "=== Post 3 with dist=0.77: Imaging databases store images.\n",
        "\n",
        "=== Post 4 with dist=0.77: Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
        "\n",
        "Best post is 3 with dist=0.77\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# normalize sample\n",
      "tmp_post_vec = X_train.getrow(4)\n",
      "tmp_normalized = tmp_post_vec / sp.linalg.norm(tmp_post_vec.toarray())\n",
      "print tmp_post_vec\n",
      "print tmp_post_vec.toarray()\n",
      "print tmp_normalized.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 8)\t3\n",
        "  (0, 4)\t3\n",
        "  (0, 7)\t3\n",
        "  (0, 20)\t3\n",
        "[[0 0 0 0 3 0 0 3 3 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n",
        "[[ 0.   0.   0.   0.   0.5  0.   0.   0.5  0.5  0.   0.   0.   0.   0.   0.\n",
        "   0.   0.   0.   0.   0.   0.5  0.   0.   0. ]]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stop words\n",
      "vectorizer = CountVectorizer(min_df=1, stop_words='english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(vectorizer.get_stop_words())[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "['a',\n",
        " 'about',\n",
        " 'above',\n",
        " 'across',\n",
        " 'after',\n",
        " 'afterwards',\n",
        " 'again',\n",
        " 'against',\n",
        " 'all',\n",
        " 'almost',\n",
        " 'alone',\n",
        " 'along',\n",
        " 'already',\n",
        " 'also',\n",
        " 'although',\n",
        " 'always',\n",
        " 'am',\n",
        " 'among',\n",
        " 'amongst',\n",
        " 'amoungst']"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(posts)\n",
      "num_samples, num_features = X_train.shape\n",
      "print(\"#samples: %d, #features: %d\" %(num_samples, num_features))\n",
      "\n",
      "# sample text contains 7 stop words."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 5, #features: 15\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_post = \"imaging databases\"\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "\n",
      "def dist_norm(v1, v2):\n",
      "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
      "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
      "    delta = v1_normalized - v2_normalized\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "best_doc = None\n",
      "best_dist = sys.maxint\n",
      "best_i = None\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist_norm(post_vec, new_post_vec)\n",
      "    print \"=== Post %i with dist=%.2f: %s\" % (i, d, post)\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "        \n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "\n",
        "=== Post 1 with dist=0.61: Imaging databases can get huge.\n",
        "\n",
        "=== Post 2 with dist=0.86: Most imaging databases safe images permanently.\n",
        "\n",
        "=== Post 3 with dist=0.77: Imaging databases store images.\n",
        "\n",
        "=== Post 4 with dist=0.77: Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
        "\n",
        "Best post is 1 with dist=0.61\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stemming\n",
      "import nltk.stem\n",
      "s = nltk.stem.SnowballStemmer('english')\n",
      "s.stem(\"graphics\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "u'graphic'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "class StemmedCountVectorizer(CountVectorizer):\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "        \n",
      "stem_vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stem_train = stem_vectorizer.fit_transform(posts)\n",
      "stem_num_samples, stem_num_features = stem_train.shape\n",
      "print(\"#samples: %d, #features: %d\" %(stem_num_samples, stem_num_features))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 5, #features: 14\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_post = \"imaging databases\"\n",
      "stem_new_post_vec = stem_vectorizer.transform([new_post])\n",
      "\n",
      "\n",
      "best_doc = None\n",
      "best_dist = sys.maxint\n",
      "best_i = None\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = stem_train.getrow(i)\n",
      "    d = dist_norm(post_vec, stem_new_post_vec)\n",
      "    print \"=== Post %i with dist=%.2f: %s\" % (i, d, post)\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "        \n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "\n",
        "=== Post 1 with dist=0.61: Imaging databases can get huge.\n",
        "\n",
        "=== Post 2 with dist=0.63: Most imaging databases safe images permanently.\n",
        "\n",
        "=== Post 3 with dist=0.52: Imaging databases store images.\n",
        "\n",
        "=== Post 4 with dist=0.52: Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
        "\n",
        "Best post is 3 with dist=0.52\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "import math\n",
      "def tfidf(term, doc, docset):\n",
      "    tf = float(doc.count(term)) / sum(doc.count(w) for w in set(doc))\n",
      "    idf = math.log(float(len(docset)) / len(([doc for doc in docset if term in doc])))\n",
      "    return tf * idf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_a, doc_abb, doc_abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
      "D = [doc_a, doc_abb, doc_abc]\n",
      "print(tfidf(\"a\", doc_a, D))\n",
      "print(tfidf(\"b\", doc_abb, D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0\n",
        "0.270310072072\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (\n",
      "            english_stemmer.stem(w) for w in analyzer(doc))\n",
      "    \n",
      "tfidf_vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='englsih')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}